Master Theorem:
    It is a formula for solving recurrence relations of the form.
    T(n) = aT(n/b) +f(n)

    n = size of input
    a = no. of subproblems in the recursion
    n/b = size of each subproblem. All problems are assumed to have the same size.
    f(n) = cost of the work done outside the recursive call, 
      which includes the cost of dividing the problem and
      cost of merging the solutions

    Here, a ≥ 1 and b > 1 are constants, and f(n) is an asymptotically positive function.

    It is used in calculating the time complexity of recurrence relation (divide and conquer algorithms) in a simple and quick way.

    T(n) = aT(n/b) + f(n)
    where, T(n) has the following asymptotic bounds:

        1. If f(n) = O(nlogb a-ϵ), then T(n) = Θ(nlogb a).

        2. If f(n) = Θ(nlogb a), then T(n) = Θ(nlogb a * log n).

        3. If f(n) = Ω(nlogb a+ϵ), then T(n) = Θ(f(n)).
    ϵ > 0 is a constant.

    Limitation:
        This theorem can't be used if 
            1. T(n) is not monotone. eg. T(n) = sin n
            2. f(n) is not a polynomial. eg. f(n) = 2^n
            3. a is not a constant. eg. a = 2n
            4. a < 1

    link: https://www.youtube.com/watch?v=FBKjvXGGCJM

        https://www.youtube.com/watch?v=2y0HQGd1-nA